{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1883637e",
   "metadata": {},
   "source": [
    "# Graded Assignment 2 – Modelling and Prediction <br>\n",
    "\n",
    "\n",
    "Dataset of reviews for cars in ``car-reviews.csv`` <br>\n",
    "Each review is labelled with either ‘Pos’ or ‘Neg’ to indicate\n",
    "whether the review has been assessed as positive or negative in the sentiment it expresses. There are 1,382 reviews in the CSV file in total, with an equal representation of both positive\n",
    "and negative classes (i.e. 691 reviews each).\n",
    "\n",
    "### Task 1 (50%): Baseline solution - Naïve Bayes classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58820566",
   "metadata": {},
   "source": [
    "Parsing the file ``car-reviews.csv``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "469444c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1382</td>\n",
       "      <td>1382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>1382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Pos</td>\n",
       "      <td>I love my 1995 Ford Explorer with automatic t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>691</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sentiment                                             Review\n",
       "count       1382                                               1382\n",
       "unique         2                                               1382\n",
       "top          Pos   I love my 1995 Ford Explorer with automatic t...\n",
       "freq         691                                                  1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('car-reviews.csv')\n",
    "df.head()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a768605",
   "metadata": {},
   "source": [
    "Let's split our dataset into train and test sets. Around 80% of the entire dataset will be used for training and 20% for testing. <br>\n",
    "Note that, the split is done randomly, since in this way we will help avoiding imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfc495af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split the x and y data\n",
    "x = np.asarray(df['Review'])\n",
    "y = np.asarray(df['Sentiment'])\n",
    "# split data into 80% train and 20% test\n",
    "# note that \"stratify=y\" will split randomly while keep the proportion of y values through the training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=4, stratify=y)\n",
    "# reshape so then can be concatenated to reform the dataframe\n",
    "\n",
    "x_train = x_train.reshape(len(x_train), 1)\n",
    "x_test = x_test.reshape(len(x_test), 1)\n",
    "y_train = y_train.reshape(len(y_train), 1)\n",
    "y_test = y_test.reshape(len(y_test), 1)\n",
    "# arrays of training data and test data choosen randomly\n",
    "arr_train = np.concatenate((y_train, x_train), axis=1)\n",
    "arr_test = np.concatenate((y_test, x_test), axis=1)\n",
    "# reconstruct the dataframe, now there are two dataframes, one for training and one for testing\n",
    "df_train = pd.DataFrame(arr_train, columns=[\"Sentiment\", \"Review\"])\n",
    "df_test = pd.DataFrame(arr_test, columns=[\"Sentiment\", \"Review\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed2db37",
   "metadata": {},
   "source": [
    "Now, a bit of exploration of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39c3b2f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Neg</td>\n",
       "      <td>A LITTLE BIT ABOUT ME FIRST  I have read man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Neg</td>\n",
       "      <td>Driving a Ford Focus Rent a car while mine is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Neg</td>\n",
       "      <td>We used to drive down the road and be jealous...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Neg</td>\n",
       "      <td>I always wanted a truck  especially after own...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pos</td>\n",
       "      <td>Captured by the styling  I have to admit  we...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sentiment                                             Review\n",
       "0       Neg    A LITTLE BIT ABOUT ME FIRST  I have read man...\n",
       "1       Neg   Driving a Ford Focus Rent a car while mine is...\n",
       "2       Neg   We used to drive down the road and be jealous...\n",
       "3       Neg   I always wanted a truck  especially after own...\n",
       "4       Pos    Captured by the styling  I have to admit  we..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81812137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1105</td>\n",
       "      <td>1105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>1105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Pos</td>\n",
       "      <td>I love my 1995 Ford Explorer with automatic t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>553</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sentiment                                             Review\n",
       "count       1105                                               1105\n",
       "unique         2                                               1105\n",
       "top          Pos   I love my 1995 Ford Explorer with automatic t...\n",
       "freq         553                                                  1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7a264be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pos</td>\n",
       "      <td>I have a 1999 Wedgewood Blue ext cab  4 door ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pos</td>\n",
       "      <td>The Mustang coupe in premium trim offers a lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Neg</td>\n",
       "      <td>I currently own an Aspire and there are some ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Neg</td>\n",
       "      <td>I purchased a 1995 Ford Contour in August  19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Neg</td>\n",
       "      <td>I bought a 1990 ford probe gl in 1998  i paye...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sentiment                                             Review\n",
       "0       Pos   I have a 1999 Wedgewood Blue ext cab  4 door ...\n",
       "1       Pos   The Mustang coupe in premium trim offers a lo...\n",
       "2       Neg   I currently own an Aspire and there are some ...\n",
       "3       Neg   I purchased a 1995 Ford Contour in August  19...\n",
       "4       Neg   I bought a 1990 ford probe gl in 1998  i paye..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff42f61e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>277</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Neg</td>\n",
       "      <td>Ford has made little or no buzz about the fac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>139</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sentiment                                             Review\n",
       "count        277                                                277\n",
       "unique         2                                                277\n",
       "top          Neg   Ford has made little or no buzz about the fac...\n",
       "freq         139                                                  1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "291a23ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples in training dataset resulting in Negative Sentiment =  552\n",
      "number of samples in training dataset resulting in Positive Sentiment =  553\n",
      "\n",
      "number of samples in testing dataset resulting in Negative Sentiment =  139\n",
      "number of samples in testing dataset resulting in Positive Sentiment =  138\n"
     ]
    }
   ],
   "source": [
    "# checking if the train and test datasets are imbalanced\n",
    "df_train_neg_review = df_train[df_train[\"Sentiment\"]=='Neg']  \n",
    "df_train_pos_review = df_train[df_train[\"Sentiment\"]=='Pos']\n",
    "print(\"number of samples in training dataset resulting in Negative Sentiment = \", df_train_neg_review.shape[0]) \n",
    "print(\"number of samples in training dataset resulting in Positive Sentiment = \", df_train_pos_review.shape[0])\n",
    "print()\n",
    "\n",
    "df_test_neg_review = df_test[df_test[\"Sentiment\"]=='Neg']  \n",
    "df_test_pos_review = df_test[df_test[\"Sentiment\"]=='Pos']\n",
    "print(\"number of samples in testing dataset resulting in Negative Sentiment = \", df_test_neg_review.shape[0]) \n",
    "print(\"number of samples in testing dataset resulting in Positive Sentiment = \", df_test_pos_review.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a88b431",
   "metadata": {},
   "source": [
    "From this quick analysis we can deduce that:\n",
    "- the total dataset has been correctly splitted into 80% training and 20% testing data\n",
    "- training and testing datasets are both balanced, so no need to apply special techniques to deal with imbalanced data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8264d0f1",
   "metadata": {},
   "source": [
    "Preprocessing\n",
    "- Redundant token removal. Identifying and excluding all punctuation and common\n",
    "words that are not likely to affect sentiment.\n",
    "- Redundant formatting removal. Ensuring that remaining words are not case-sensitive\n",
    "i.e. the classifier should not distinguish upper/lower case characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f52c80cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "class PreprocessVectorization:\n",
    "    def __init__(self):\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "        self.data = 0\n",
    "        self.target = 0\n",
    "        self.clean_stopwords_arr = 0\n",
    "        self.cleanpunct_arr = 0\n",
    "        self.lowercase_arr = 0\n",
    "        self.stemmed_arr = 0\n",
    "        self.unstemmed = 0\n",
    "        self.stemmed = 0\n",
    "        self.wordset_arr = 0\n",
    "        self.word_to_idx = 0\n",
    "        self.idx_to_word = 0\n",
    "        self.bow_arr = 0\n",
    "        self.target_encode = 0\n",
    "    \n",
    "    def set_data(self, target, data):\n",
    "        \"\"\"Collect the data in input\"\"\"\n",
    "        self.target = target\n",
    "        self.data = data\n",
    "        \n",
    "        \n",
    "    # function to identify the stopw words within the sample, delete them and returning the cleaned sentence\n",
    "    def clean_stopwords(self):\n",
    "        \"\"\"\n",
    "        For feature reduction, since they will not add any value to the classification,\n",
    "        deleting stop-words like and, or, etc.\n",
    "        \"\"\"\n",
    "        in_arr = self.data\n",
    "        self.clean_stopwords_arr=[]\n",
    "        for sentence in in_arr:\n",
    "            tmp_list=[]\n",
    "            for word in sentence.split():\n",
    "                if word.lower() not in self.stopwords:\n",
    "                    tmp_list.append(word)\n",
    "            self.clean_stopwords_arr.append(' '.join(tmp_list))\n",
    "            \n",
    "    def clean_punctuation(self):\n",
    "        \"\"\"\n",
    "        For feature reduction, since they will not add any value to the classification,\n",
    "        removing special characters/punctuations like !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~    \n",
    "        \"\"\"\n",
    "        # first, split into words based on spacing\n",
    "        in_arr = self.clean_stopwords_arr\n",
    "        splitted_arr = np.empty(shape=(len(in_arr), 1), dtype=object)\n",
    "        for row in range(len(in_arr)):\n",
    "            splitted_arr[row] = np.char.split(in_arr[row])\n",
    "        # remove special characters/punctuations\n",
    "        in_arr = np.copy(splitted_arr)\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        self.cleanpunct_arr = np.empty(shape=(len(in_arr), 1), dtype=object)\n",
    "        in_arr = np.reshape(in_arr, (len(in_arr), 1))\n",
    "        for row in range(len(in_arr)):\n",
    "            self.cleanpunct_arr[row, 0] = [w.translate(table) for w in in_arr[row][0][:]]\n",
    "    \n",
    "    def lower_case(self):\n",
    "        \"\"\"\n",
    "        For feature reduction, since they will not add any value to the classification,\n",
    "        words are converted to lower character.\n",
    "        \"\"\"\n",
    "        # taking care of the case-sensitive words\n",
    "        in_arr = self.cleanpunct_arr\n",
    "        self.lowercase_arr = np.empty(shape=(len(in_arr), 1), dtype=object)\n",
    "        for row in range(len(in_arr)):\n",
    "            self.lowercase_arr[row, 0] = [w.lower() for w in in_arr[row][0][:]]\n",
    "    \n",
    "    def stemming(self):\n",
    "        \"\"\"\n",
    "        For feature reduction, reducing the words to their stem or root only.\n",
    "        e.g. before stemming:\n",
    "        'complete', 'completely', 'completly', 'completed', 'completion', 'completing', 'completes'\n",
    "        after stemming are reduced to only:\n",
    "        'complet'\n",
    "        \"\"\"\n",
    "        snow_stemmer = SnowballStemmer(language='english')\n",
    "        in_arr = self.lowercase_arr\n",
    "        temp_stemmed_arr=[]\n",
    "        for row in range(len(in_arr)):\n",
    "            stem_words = []  \n",
    "            for word in in_arr[row][0][:]:\n",
    "                val = snow_stemmer.stem(word)\n",
    "                stem_words.append(val)\n",
    "            temp_stemmed_arr.append(' '.join(stem_words))\n",
    "        \n",
    "        # the column 'Review' now including words filtered by their stems\n",
    "        self.stemmed_arr = np.empty(shape=(len(temp_stemmed_arr), 1), dtype=object)\n",
    "        for row in range(len(temp_stemmed_arr)):\n",
    "            self.stemmed_arr[row] = np.char.split(temp_stemmed_arr[row])  \n",
    " \n",
    "\n",
    "    def union_calc(self, in_arr):\n",
    "        \"\"\"\n",
    "        Recursive method to find unique words by checking them in couples\n",
    "        \"\"\"\n",
    "        if len(in_arr) == 1:\n",
    "            return in_arr[0][0]\n",
    "        else:\n",
    "            return np.union1d( in_arr[0][0], self.union_calc(in_arr[1:]) )  \n",
    "        \n",
    "    \n",
    "    def wordset_calc(self):\n",
    "        \"\"\"\n",
    "        Receiving in input the array (num samples x 1) of sentences\n",
    "        Returning as output:\n",
    "        > the array containing the group of unique words which have been found in the entire dataset\n",
    "        > the array to convert word_set_arr from word into index\n",
    "        > the array to convert word_set_arr from index into word\n",
    "        \"\"\"\n",
    "\n",
    "        in_arr = self.stemmed_arr\n",
    "            \n",
    "        self.wordset_arr = self.union_calc(in_arr)        \n",
    "        self.word_to_idx = {}\n",
    "        self.idx_to_word = {}\n",
    "        for idx, word in enumerate(self.wordset_arr):\n",
    "            # Set the mapping indexes.\n",
    "            self.word_to_idx[word] = idx\n",
    "            self.idx_to_word[idx] = word\n",
    "        return self.wordset_arr\n",
    "            \n",
    "    def convert_to_BoW(self, sentence):\n",
    "        \"\"\"\n",
    "        Receiving in input:\n",
    "        > sentence: the text\n",
    "        > wordset_arr: the array containing the group of unique words which have been found in the entire dataset\n",
    "        > word_to_idx: dictionary that converts word to index\n",
    "        Giving back as output:\n",
    "        Numerical count of the number of times each word/stem appears in that sample\n",
    "        \"\"\"\n",
    "        converted_bow = np.zeros(len(self.wordset_arr))\n",
    "        for word in sentence:\n",
    "            # Iterate over sentence words checking if they are in the vocabulary.\n",
    "            if word in self.wordset_arr:\n",
    "                # Change the value of that specific index, by increasing the value.\n",
    "                converted_bow[self.word_to_idx[word]]+=1  \n",
    "        converted_bow = np.reshape(converted_bow, (len(self.wordset_arr), 1))\n",
    "        converted_bow = np.transpose(converted_bow)\n",
    "        return converted_bow\n",
    "    \n",
    "    \n",
    "    def BoW_creation(self):\n",
    "        \"\"\"\n",
    "        Applying convert_toBoW to each row/sample\n",
    "        \"\"\"\n",
    "        # Let's go through each sentence, so each sample of the dataset\n",
    "        in_arr = self.stemmed_arr\n",
    "        self.bow_arr = np.empty((len(in_arr), len(self.wordset_arr)))\n",
    "        for row, sentence in enumerate(in_arr):\n",
    "            # Substitute each row by the sentence BoW.\n",
    "            self.bow_arr[row] = self.convert_to_BoW(sentence[0])\n",
    "        return self.bow_arr[0]\n",
    "    \n",
    "    \n",
    "    def encoder(self):\n",
    "        encoder = OrdinalEncoder()\n",
    "        \"\"\"Convert 'Neg' into 0 and 'Pos' into 1\"\"\" \n",
    "        reshape_target = np.reshape(self.target, (len(self.target), 1))\n",
    "        self.target_encode = encoder.fit_transform(reshape_target)\n",
    "        \n",
    "        \n",
    "    def process(self):\n",
    "        \"\"\"\n",
    "        Calling the methods in a certain order to perform the required steps to process the data\n",
    "        \"\"\"\n",
    "        self.clean_stopwords()\n",
    "        self.clean_punctuation()\n",
    "        self.lower_case()\n",
    "        self.stemming()\n",
    "        self.wordset_calc()\n",
    "        self.BoW_creation()\n",
    "        self.encoder()\n",
    "        # returning the encoded label column for \"Sentiment\",\n",
    "        # the bag of words vector per each sample\n",
    "        # the features / words name for each column of the bow\n",
    "        return self.target_encode, self.bow_arr, self.wordset_arr\n",
    "    \n",
    "    \n",
    "    def check_stemming(self, word_root):\n",
    "        \"\"\"\n",
    "        Checking tree roots or stem words to see if the method 'stemming' worked.\n",
    "        \"\"\"\n",
    "        in_arr = self.lowercase_arr\n",
    "        self.unstemmed = []\n",
    "        for row in range(len(in_arr)):\n",
    "            for word in in_arr[row][0][:]:\n",
    "                if word.startswith(word_root) and (word not in self.unstemmed):\n",
    "                    self.unstemmed.append(word)\n",
    "\n",
    "        self.stemmed = []\n",
    "        for row in range(len(self.stemmed_arr)):\n",
    "            for word in self.stemmed_arr[row][0][:]:\n",
    "                if word.startswith(word_root) and (word not in self.stemmed):\n",
    "                    self.stemmed.append(word)  \n",
    "        return self.unstemmed, self.stemmed\n",
    "    \n",
    "        \n",
    "    def __str__(self, word_root):\n",
    "        res_check_stemming = self.check_stemming(word_root)\n",
    "        unstemmed = res_check_stemming[0]\n",
    "        stemmed = res_check_stemming[1]\n",
    "        return f'Unstemmed Words {unstemmed} become stemmed into {stemmed}'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9c0542",
   "metadata": {},
   "source": [
    "Vectorization of the training set into bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1f9fcf8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unstemmed Words ['complete', 'completely', 'completly', 'completed', 'completion', 'completing', 'completes'] become stemmed into ['complet']\n",
      "\n",
      "Unstemmed Words ['features', 'feature', 'featured', 'featurs', 'featuring'] become stemmed into ['featur']\n",
      "\n",
      "Unstemmed Words ['replace', 'replaced', 'replacement', 'replacing', 'replacements', 'replaces'] become stemmed into ['replac']\n",
      "\n",
      "\n",
      "This is the first row/sample bag of words only showing words/features from 1250 to 1650:\n",
      "\n",
      "['anxieti' 'anxious' 'anybodi' 'anyday' 'anyhow' 'anymor' 'anyon'\n",
      " 'anyquest' 'anyth' 'anytim' 'anyway' 'anywher' 'aol' 'apal' 'apar'\n",
      " 'apart' 'apathi' 'apiec' 'apolog' 'apologet' 'appar' 'appeal' 'appear'\n",
      " 'appeas' 'appl' 'appli' 'applianc' 'applic' 'appoint' 'appreci'\n",
      " 'approach' 'appropri' 'approv' 'approx' 'approxim' 'apr' 'april' 'aprox'\n",
      " 'aprrox' 'apt' 'aqua' 'aquarium' 'aquir' 'ar' 'arbitr' 'arc' 'arch'\n",
      " 'archangel' 'architect' 'arctic' 'ard' 'area' 'arena' 'arent' 'areostar'\n",
      " 'argent' 'argh' 'argu' 'arguabl' 'argument' 'aris' 'arizona' 'arkansa'\n",
      " 'arm' 'armi' 'armor' 'armrest' 'arnt' 'around' 'aroung' 'arrang' 'array'\n",
      " 'arriv' 'arrow' 'art' 'arti' 'articl' 'artifici' 'artist' 'asap' 'ascend'\n",
      " 'ascent' 'ascertain' 'ash' 'asham' 'ashburn' 'ashtray' 'asian' 'asid'\n",
      " 'ask' 'asknig' 'asleep' 'asp' 'aspca' 'aspect' 'aspen' 'asphalt'\n",
      " 'asphault' 'aspir' 'ass' 'assail' 'assassin' 'assembl' 'assert' 'assess'\n",
      " 'asset' 'assi' 'assign' 'assist' 'assoc' 'associ' 'assort' 'assum'\n",
      " 'assumpt' 'assur' 'asthet' 'asthma' 'astonish' 'astound' 'astray' 'astro'\n",
      " 'at' 'ate' 'atf' 'athlet' 'atlanta' 'atleast' 'atmospher' 'aton' 'atop'\n",
      " 'atribut' 'atroci' 'att' 'attach' 'attack' 'attain' 'attempt' 'attena'\n",
      " 'attend' 'attent' 'attest' 'attic' 'attitud' 'attorney' 'attract'\n",
      " 'attribut' 'attun' 'atv' 'atx' 'atyp' 'auction' 'audac' 'audi' 'audibl'\n",
      " 'audienc' 'audio' 'audiophil' 'aug' 'august' 'aunt' 'aura' 'aussi'\n",
      " 'austin' 'australia' 'australian' 'author' 'auto' 'autobahn' 'autobytel'\n",
      " 'autolock' 'autom' 'automak' 'automat' 'automobil' 'automot' 'autonomi'\n",
      " 'autosafeti' 'autoshow' 'autoweek' 'autumn' 'auxileri' 'auxiliari'\n",
      " 'auxilliari' 'avail' 'availi' 'avalanch' 'avella' 'aveng' 'avenu'\n",
      " 'averag' 'avg' 'avid' 'avoid' 'aw' 'await' 'awak' 'awar' 'award' 'away'\n",
      " 'awd' 'awe' 'awesom' 'awestruck' 'awhil' 'awkward' 'awri' 'awsom' 'awwww'\n",
      " 'ax4n' 'axal' 'axel' 'axl' 'axod' 'aztec' 'b' 'b2' 'b2000' 'b3000'\n",
      " 'b4000' 'baaction' 'babi' 'babyseat' 'bachelor' 'back' 'backdat'\n",
      " 'backend' 'backfield' 'background' 'backlight' 'backpack' 'backrest'\n",
      " 'backseat' 'backsid' 'backup' 'backward' 'backyard' 'bad' 'badder'\n",
      " 'baddest' 'badg' 'baffl' 'bag' 'baggag' 'baggi' 'bahama' 'baja' 'bake'\n",
      " 'baker' 'balanc' 'bald' 'bale' 'ball' 'ballast' 'bam' 'ban' 'banana'\n",
      " 'band' 'bandi' 'bandwagon' 'bane' 'bang' 'banger' 'bank' 'bankruptci'\n",
      " 'bankston' 'baout' 'bar' 'barb' 'barcaloung' 'bare' 'bareabl' 'barest'\n",
      " 'barg' 'bargain' 'bargiend' 'bark' 'barley' 'barn' 'barney' 'barnyard'\n",
      " 'barrel' 'barren' 'barrier' 'base' 'basebal' 'baselin' 'basement' 'bash'\n",
      " 'basi' 'basic' 'basket' 'basketbal' 'bass' 'bat' 'batch' 'bath' 'batman'\n",
      " 'batteri' 'battl' 'battletank' 'bauer' 'baulki' 'bay' 'bazooka' 'bd3'\n",
      " 'be' 'beach' 'bead' 'beam' 'bean' 'bear' 'bearabl' 'beast' 'beasti'\n",
      " 'beat' 'beaten' 'beater' 'beauti' 'bebop' 'becam' 'because3' 'beccam'\n",
      " 'beck' 'becom' 'becuas' 'bed' 'bedlin' 'bedroom' 'beef' 'beefcak' 'beefi'\n",
      " 'beefier' 'beent' 'beep' 'beeper' 'beer' 'beetl' 'beforehand' 'beg'\n",
      " 'began' 'beggar' 'begin' 'begrudg' 'begun' 'behalf' 'behav' 'behavior'\n",
      " 'behemoth' 'behind' 'behold' 'beig' 'beij' 'belabor' 'belfast' 'beli'\n",
      " 'belief' 'believ' 'beliv' 'bell' 'belli' 'bellyach' 'belong' 'belov'\n",
      " 'belt' 'beltlin' 'beltway' 'bench' 'bend' 'bender' 'beneath' 'benefici'\n",
      " 'benefit' 'bent' 'bentley' 'benz' 'berber' 'beretta' 'besid' 'best'\n",
      " 'bestsel' 'bet' 'betray' 'better' 'betwe' 'betwixt' 'beverag' 'bewar'\n",
      " 'bewilder' 'beyond' 'bfg' 'bgoodday' 'bhp' 'bi' 'bias' 'bicycl' 'big'\n",
      " 'bigblock' 'bigge' 'bigger' 'biggest']\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 6. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# creating the PV object\n",
    "PV = PreprocessVectorization()\n",
    "# giving as inputs the training dataset\n",
    "target = np.asarray(df_train['Sentiment'])\n",
    "data = np.asarray(df_train['Review'])\n",
    "PV.set_data(target, data)\n",
    "\n",
    "# execute the process to vectorize the training dataset\n",
    "[train_target_encode, train_bow_arr, train_wordset_arr] = PV.process()\n",
    "\n",
    "# testing the stemming functionality\n",
    "words_root = ['complet', 'featur', 'replac']\n",
    "for word_root in words_root:\n",
    "    print(PV.__str__(word_root))\n",
    "    print()\n",
    "\n",
    "# testing the vectorization to generate the features\n",
    "print()\n",
    "print(\"This is the first row/sample bag of words only showing words/features from 1250 to 1650:\")\n",
    "print()\n",
    "bow_words_res = PV.wordset_calc()\n",
    "print(bow_words_res[1250:1650])\n",
    "bow_values_res = PV.BoW_creation()\n",
    "print(bow_values_res[1250:1650])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730e5726",
   "metadata": {},
   "source": [
    "Vectorization of the test set. <br>\n",
    "Data leakage is carefully considered. For this reason, the bag of words vectorization solely obtained by the use of training data, it is used as the structure for the testing data vectorization. In practise, only words/features detected by the vectorization of the training data are considered, without affecting them by the knowledge of the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da7ed957",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# giving as inputs the testing dataset\n",
    "target = np.asarray(df_test['Sentiment'])\n",
    "data = np.asarray(df_test['Review'])\n",
    "PV.set_data(target, data)\n",
    "\n",
    "# execute the process to vectorize the testing dataset\n",
    "[test_target_encode, test_bow_arr, test_wordset_arr] = PV.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8eacb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dictionaries to map word to index\n",
    "train_word_to_idx = {}\n",
    "for idx, word in enumerate(train_wordset_arr):\n",
    "    # Set the mapping indexes.\n",
    "    train_word_to_idx[word] = idx\n",
    "    \n",
    "test_word_to_idx = {}\n",
    "for idx, word in enumerate(test_wordset_arr):\n",
    "    # Set the mapping indexes.\n",
    "    test_word_to_idx[word] = idx\n",
    "\n",
    "# fitting the test bag of words into the features vectorization structure obtained with the training bag of words\n",
    "test_bow_compatib_arr = 0 * train_bow_arr[0: len(test_bow_arr), :]\n",
    "test_wordset_compatib_arr = train_wordset_arr\n",
    "for w_train in train_wordset_arr:\n",
    "    for w_test in test_wordset_arr:\n",
    "        if w_train == w_test:\n",
    "            test_bow_compatib_arr[:, train_word_to_idx[w_train]] = test_bow_arr[:, test_word_to_idx[w_test]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa49b18",
   "metadata": {},
   "source": [
    "Let's train a machine learning model based on Naive Bayes.<br>\n",
    "Since the features vector has elements representing the number of times that 'word' appears (it's frequency), then according to wikipedia (2022) Bernoulli Naive Bayes approach (used when a word is present or not) is discarded in favour of the ***Multinomial Naive Bayes***. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c3f7128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "<class 'numpy.ndarray'>\n",
      "[0. 1.]\n",
      "[1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1.\n",
      " 1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0.\n",
      " 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1.\n",
      " 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 1.\n",
      " 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1.\n",
      " 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1.\n",
      " 0. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 1. 1.\n",
      " 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0.\n",
      " 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\damat\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:985: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# machine learning model training phase\n",
    "# feature matrix\n",
    "X = train_bow_arr\n",
    "# target label vector\n",
    "y = train_target_encode\n",
    "# fitting the machine learning model\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X, y)\n",
    "\n",
    "# machine learning model prediction phase\n",
    "y_pred = clf.predict(test_bow_compatib_arr)\n",
    "y_true = np.ravel(test_target_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043a22fa",
   "metadata": {},
   "source": [
    "Machine Learning model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a2fd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "conf_mat = confusion_matrix(y_true, y_pred, labels=clf.classes_)\n",
    "disp_obj = ConfusionMatrixDisplay(confusion_matrix=conf_mat, display_labels=['Neg','Pos'])\n",
    "\n",
    "disp_obj.plot()\n",
    "\n",
    "disp_obj.ax_.set(title='Confusion Matrix: Car Review Prediction with Multinomial Naive Bayes',\n",
    "               xlabel='Predicted Sentiment' ,\n",
    "                ylabel='True Sentiment')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c49e63",
   "metadata": {},
   "source": [
    "Observations about the Multinomial Naive Bayes Machine Learning Model prediction capabilities\n",
    "- Accuracy: this model is capable of predicting the right Sentiment 80% of the total observations (sum of the confusion matrix diagonal values, divided by all values).\n",
    "- Confusion Matrix: clearly shows that the diagonal take most of the values, hence high accuracy. It also shows that the model is a bit better in predicting 'Pos' Sentiment, and also a little bit prone to have higher False Positive than False Negative. This is obviously not auspicable, since this would produce good predicted reviews where instead the reviews were bad.\n",
    "```\n",
    "    - True Negative = 108\n",
    "    - True Positive = 113\n",
    "    - False Negative (Type II error) = 25\n",
    "    - False Positive (Type I error) = 31\n",
    "```\n",
    "All this is also summarised by the precision and recall results <br>\n",
    "\n",
    "Looking at F1-score, it is very similar to the accuracy, this is beacuse the data in test are balanced, around same number of 'Pos' vs. 'Neg' samples.\n",
    "\n",
    "Now, let's have a look at how the model performs on seen data, so training data, the very same data that have been used to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a5e389",
   "metadata": {},
   "source": [
    "### Task 2 (50%): Improved solution\n",
    "Identifying a better approach suitable for this specific task of binary sentiment classification of text-based reviews.<br>\n",
    "According to C. Manning (2009), stemming has the main limitation that it returns a stem that migth or might not be representating a meaningful word, whereas ***lemmatization*** can help avoiding this issue, since it always returns meaningful words.<br>\n",
    "According to N. S. Joshi (2014), SVM has outperformed other classifiers such as Naïve Bayes for text classification.<br>\n",
    "Let's train a machine learning model based on ***Support Vector Classifier (SVC)***.<br>\n",
    "Note that this is also suggested by the scikit learn diagram https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc6eb77",
   "metadata": {},
   "source": [
    "First, let's pre-preprocess the text, in particular applying spelling correction, cleaning of double spaces and finally lemmatizing (note that lemmatizing takes time to be run, wound 10min on normal laptop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b22403e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\damat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from spellchecker import SpellChecker\n",
    "spell = SpellChecker()\n",
    "\n",
    "\n",
    "def spell_correct(text):\n",
    "    \"\"\"\n",
    "    Correcting the spelling of each word within the text\n",
    "    \"\"\"\n",
    "    spell_list = []\n",
    "    for sentence in text:\n",
    "        spell_sentence = []\n",
    "        for word in sentence.split():\n",
    "            spell_sentence.append(spell.correction(word))\n",
    "        spell_list.append(' '.join(spell_sentence))\n",
    "        \n",
    "    return spell_list\n",
    "    \n",
    "    \n",
    "def delete_double_space(text):\n",
    "    \"\"\"\n",
    "    Substituting double space with single space\n",
    "    \"\"\"\n",
    "    for i in range(len(text)):\n",
    "        text[i] = re.sub(' +', ' ', text[i])\n",
    "    return text\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def lemmatizing(text):\n",
    "    \"\"\"\n",
    "    Converting a word to its base form, while considering the context it converts the word to \n",
    "    its meaningful base form.\n",
    "    \"\"\"\n",
    "    for row in range(len(text)):\n",
    "        sentence = word_tokenize(text[row])\n",
    "        sentence = [lemmatizer.lemmatize(word.lower()) for word in sentence if word not in set(stopwords.words('english'))]\n",
    "        text[row]=' '.join(sentence)\n",
    "    return text\n",
    "\n",
    "\n",
    "x_train_list = df_train['Review'].tolist()\n",
    "x_test_list = df_test['Review'].tolist()\n",
    "\n",
    "# applying spelling correction\n",
    "x_train_list = spell_correct(x_train_list) \n",
    "x_test_list = spell_correct(x_test_list)\n",
    "# applying double space deletion\n",
    "x_train_list = delete_double_space(x_train_list) \n",
    "x_test_list = delete_double_space(x_test_list)\n",
    "# applying lemmatization\n",
    "x_train_list = lemmatizing(x_train_list)\n",
    "x_test_list = lemmatizing(x_test_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a719c24",
   "metadata": {},
   "source": [
    "Here using the Tf-idf method which stands for term frequency-inverse document frequency. This will combine the frequency of a word in a specific sample with the frequency of that word within the entire document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35296d5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TfidfVectorizer should automatically vectorize text data. The resulting features are necessary structure for ML\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# using min_df=3 to ignore features that appear in less than 3 samples\n",
    "tfidfvectorizer = TfidfVectorizer(ngram_range=(1, 1), min_df=0)\n",
    "\n",
    "X = tfidfvectorizer.fit_transform(x_train_list)\n",
    "train_bow_arr = X.toarray()\n",
    "\n",
    "# getting the features names for the training dataset\n",
    "train_wordset_arr = tfidfvectorizer.get_feature_names()\n",
    "\n",
    "X = tfidfvectorizer.fit_transform(x_test_list)\n",
    "test_bow_arr = X.toarray()\n",
    "# getting the features names for the testing dataset\n",
    "test_wordset_arr = tfidfvectorizer.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62640e5",
   "metadata": {},
   "source": [
    "Second, let's deploy Support Vector Vachine Vlassifier with grid search method to optimise the model hyperparameters. <br>\n",
    "\n",
    "User:\n",
    "- please choose `already_optimised = 1` if you prefer to use the SVM best hyperparameters (previously calculated)\n",
    "- please choose `already_optimised = 0` if you prefer to run the grid search to determine the best hyperparameters (it requires few minutes to complete) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188c301b",
   "metadata": {},
   "outputs": [],
   "source": [
    "already_optimised = 1\n",
    "\n",
    "# creating dictionaries to map word to index\n",
    "train_word_to_idx = {}\n",
    "for idx, word in enumerate(train_wordset_arr):\n",
    "    # Set the mapping indexes.\n",
    "    train_word_to_idx[word] = idx\n",
    "    \n",
    "test_word_to_idx = {}\n",
    "for idx, word in enumerate(test_wordset_arr):\n",
    "    # Set the mapping indexes.\n",
    "    test_word_to_idx[word] = idx\n",
    "\n",
    "# fitting the test bag of words into the features vectorization structure obtained with the training bag of words\n",
    "test_bow_compatib_arr = 0 * train_bow_arr[0: len(test_bow_arr), :]\n",
    "test_wordset_compatib_arr = train_wordset_arr\n",
    "for w_train in train_wordset_arr:\n",
    "    for w_test in test_wordset_arr:\n",
    "        if w_train == w_test:\n",
    "            test_bow_compatib_arr[:, train_word_to_idx[w_train]] = test_bow_arr[:, test_word_to_idx[w_test]]\n",
    "            \n",
    "\n",
    "from sklearn.svm import SVC\n",
    "# machine learning model training phase\n",
    "# feature matrix\n",
    "X = train_bow_arr\n",
    "# target label vector\n",
    "y = train_target_encode\n",
    "# fitting the machine learning model\n",
    "clf = SVC()\n",
    "\n",
    "# using gridsearch to find the best SVM model hyperparameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# selecting if o use already optimised hyperparameters or if to run the grid search\n",
    "if already_optimised == 1:\n",
    "    param_grid = {'C': [1], 'gamma': [1], 'kernel': ['linear']}\n",
    "else:\n",
    "    param_grid = {'C':[1,10,100,1000],'gamma':[1,0.1,0.001,0.0001], 'kernel':['linear','rbf']}\n",
    "\n",
    "\n",
    "grid = GridSearchCV(SVC(),param_grid,refit = True, verbose=2)\n",
    "\n",
    "grid.fit(X, y)\n",
    "\n",
    "print(grid.best_params_)\n",
    "# machine learning model prediction phase\n",
    "y_pred = grid.predict(test_bow_compatib_arr)\n",
    "y_true = np.ravel(test_target_encode)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "conf_mat = confusion_matrix(y_true, y_pred, labels=grid.classes_)\n",
    "disp_obj = ConfusionMatrixDisplay(confusion_matrix=conf_mat, display_labels=['Neg','Pos'])\n",
    "\n",
    "disp_obj.plot()\n",
    "\n",
    "disp_obj.ax_.set(title='Confusion Matrix: Car Review Prediction with Support Vector Machine Classifier',\n",
    "               xlabel='Predicted Sentiment' ,\n",
    "                ylabel='True Sentiment')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5707a51f",
   "metadata": {},
   "source": [
    "According to Uiterkamp (2019), N-grams are able to take some context into account when evaluating classes, specifically n words of context. However, this requires an extra filtering step to counter, such as only allowing bigrams that occur at least N amount of times in the training dataset to be added to the vocabulary. Additionally, the probability of encountering the bigrams in new data is smaller than encountering unigrams, which means a larger vocabulary is required to capture a decent number of bigrams in a new text. These problems end up increasing tremendously the size of the vocabulary as n in the n-grams increases.<br>\n",
    "Here following a bi-grams (N-grams with N=2). Here using the Tf-idf method which stands for term frequency-inverse document frequency. This will combine the frequency of a word in a specific sample with the frequency of that word within the entire document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e41391",
   "metadata": {},
   "source": [
    "In conclusion, the use of lemmatization and SVM with optimised hyperparameters have improved the overall ML model accuracy with test data but only marginally, around 1%.<br>\n",
    "According to P. Barba (2019), 'When evaluating the sentiment (positive, negative, neutral) of a given text document, research shows that human analysts tend to agree around 80-85% of the time'. This implies that, with a text made of unstructured reviews like the one in input, we might have potentalially hit a limit around 81%, and so we could only improve it slightly with a very high cost in implementation complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737c9a16",
   "metadata": {},
   "source": [
    "### Reference List\n",
    "- Neha S. Joshi , Suhasini A. Itkat., 2014, *A Survey on Feature Level Sentiment Analysis.* https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.644.1418&rep=rep1&type=pdf\n",
    "- Wikipedia, Wikimedia Foundation, *Naive Bayes classifier*, 4 March 2022, https://en.wikipedia.org/wiki/Naive_Bayes_classifier\n",
    "- Luc Schoot Uiterkamp, August 2019, *Improving text representations for NLP from bags of words to strings of words*, http://essay.utwente.nl/79245/1/Schoot%20Uiterkamp_MA_BMS.pdf\n",
    "- Christopher D. Manning, 2009, *An Introduction to Information Retrieval*, https://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf\n",
    "- Paul Barba, 2019, *Sentiment Accuracy: Explaining the Baseline and How to Test It*, https://www.lexalytics.com/lexablog/sentiment-accuracy-baseline-testing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
